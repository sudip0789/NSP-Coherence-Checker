{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "73153da7-05ea-47dc-a725-3e29467b526c",
      "metadata": {
        "id": "73153da7-05ea-47dc-a725-3e29467b526c"
      },
      "source": [
        "# Coherence Checker Using Multi-Approach Based NSP\n",
        "\n",
        "\n",
        "Done by: Sudip Das\n",
        "\n",
        "Last Updated: May 7th, 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df84be70-5f8c-4118-a580-48d2351f5164",
      "metadata": {
        "id": "df84be70-5f8c-4118-a580-48d2351f5164"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Learning a new language can be particularly challenging for young children and non-native English speakers, especially when it comes to constructing coherent sentences. To assist with this process, we leverage Next Sentence Prediction (NSP) as a tool for evaluating the coherence of user-generated sentences. By doing so, we can provide immediate feedback and improve their understanding of English grammar and syntax.\n",
        "\n",
        "This project is structured into multiple phases, each designed to explore different approaches for improving sentence coherence detection. The primary objectives include:\n",
        "\n",
        "- Fine-tuning a pre-trained model on a subset of the Wikipedia dataset for enhanced understanding of coherent sentence pairs.\n",
        "\n",
        "- Experimenting with various prompting methods, including standard and few-shot prompts, to assess how well the model can identify coherence.\n",
        "\n",
        "- Implementing a retrieval-augmented generation (RAG) approach that utilizes a FAISS index built from coherent and incoherent sentence pairs.\n",
        "\n",
        "- Combining above approaches to achieve higher accuracy in NSP tasks.\n",
        "\n",
        "- Demonstrating the practical benefits of these techniques for young learners, including clearer feedback on writing errors and more personalized language learning support.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad83b92-e851-4933-83a0-4fb3938ad856",
      "metadata": {
        "id": "9ad83b92-e851-4933-83a0-4fb3938ad856"
      },
      "source": [
        "## Install & Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f68fd0-a120-4cc7-8de7-d2907ffc02c9",
      "metadata": {
        "id": "35f68fd0-a120-4cc7-8de7-d2907ffc02c9",
        "outputId": "10fb8ef7-f6aa-431a-bd49-cb43945c8d41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==2.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.1.2)\n",
            "Requirement already satisfied: torchaudio==2.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.1.2)\n",
            "Requirement already satisfied: transformers==4.29.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.29.2)\n",
            "Requirement already satisfied: datasets==2.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.12.0)\n",
            "Requirement already satisfied: huggingface_hub==0.15.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.15.1)\n",
            "Requirement already satisfied: sentence-transformers==2.2.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.2.2)\n",
            "Requirement already satisfied: numpy==1.26.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.26.3)\n",
            "Requirement already satisfied: scikit-learn==1.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.4.0)\n",
            "Requirement already satisfied: pandas==2.1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.1.4)\n",
            "Requirement already satisfied: matplotlib==3.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.8.2)\n",
            "Requirement already satisfied: tqdm==4.66.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.66.1)\n",
            "Requirement already satisfied: nltk==3.8.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.8.1)\n",
            "Requirement already satisfied: faiss-cpu==1.7.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.7.4)\n",
            "Requirement already satisfied: evaluate==0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.4.1)\n",
            "Requirement already satisfied: accelerate==0.25.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.25.0)\n",
            "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.1.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.1.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.1.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.1.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.1.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.1.2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.1.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.1.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.1.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.1.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.1.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.1.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.1.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.1.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.1.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.1.2) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.1.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.1.2) (2.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.29.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.29.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.29.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.29.2) (2.32.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.29.2) (0.13.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets==2.12.0) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets==2.12.0) (0.3.6)\n",
            "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets==2.12.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets==2.12.0) (0.70.14)\n",
            "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets==2.12.0) (3.11.14)\n",
            "Requirement already satisfied: responses<0.19 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets==2.12.0) (0.18.0)\n",
            "Requirement already satisfied: torchvision in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.16.2)\n",
            "Requirement already satisfied: scipy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.11.4)\n",
            "Requirement already satisfied: sentencepiece in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.2.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn==1.4.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn==1.4.0) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas==2.1.4) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas==2.1.4) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas==2.1.4) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib==3.8.2) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib==3.8.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib==3.8.2) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib==3.8.2) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib==3.8.2) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib==3.8.2) (3.2.1)\n",
            "Requirement already satisfied: click in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk==3.8.1) (8.1.8)\n",
            "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate==0.25.0) (7.0.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate==0.25.0) (0.5.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2) (12.8.93)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.1.4) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.29.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.29.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.29.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.29.2) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch==2.1.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch==2.1.2) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "#Install dependencies\n",
        "\n",
        "!pip install \\\n",
        "  torch==2.1.2 \\\n",
        "  torchaudio==2.1.2 \\\n",
        "  transformers==4.29.2 \\\n",
        "  datasets==2.12.0 \\\n",
        "  huggingface_hub==0.15.1 \\\n",
        "  sentence-transformers==2.2.2 \\\n",
        "  numpy==1.26.3 \\\n",
        "  scikit-learn==1.4.0 \\\n",
        "  pandas==2.1.4 \\\n",
        "  matplotlib==3.8.2 \\\n",
        "  tqdm==4.66.1 \\\n",
        "  nltk==3.8.1 \\\n",
        "  faiss-cpu==1.7.4 \\\n",
        "  evaluate==0.4.1 \\\n",
        "  accelerate==0.25.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33ca1d5b-119c-481a-b3f4-206a3b972dcd",
      "metadata": {
        "id": "33ca1d5b-119c-481a-b3f4-206a3b972dcd"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Hugging Face imports\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    BertForNextSentencePrediction,\n",
        "    BertTokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    BartForConditionalGeneration,\n",
        "    BartTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import evaluate\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cee1d4b5-23e0-476b-a74c-fcbf7569f924",
      "metadata": {
        "id": "cee1d4b5-23e0-476b-a74c-fcbf7569f924",
        "outputId": "59a84a6a-d977-4f7b-f6ff-e14d70f0bad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37bcf0ae-a843-4777-a6cc-48a1d1000f35",
      "metadata": {
        "id": "37bcf0ae-a843-4777-a6cc-48a1d1000f35",
        "outputId": "e01e3736-5304-4f3c-ec63-15b72847cc88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /teamspace/studios/this_studio/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# Set your Hugging Face access token\n",
        "os.environ[\"HUGGINGFACE_TOKEN\"] = \"Enter your token here\"\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=os.environ[\"HUGGINGFACE_TOKEN\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MEk7aePbC5OE"
      },
      "id": "MEk7aePbC5OE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "541db316-7b18-492d-929b-fd04a19bc5de",
      "metadata": {
        "id": "541db316-7b18-492d-929b-fd04a19bc5de"
      },
      "source": [
        "## Extract Dataset\n",
        "\n",
        "The Wiki Paragraphs dataset used in this NSP task provides a large-scale, high-quality source of English sentence pairs for fine-tuning. By utilizing Wikipedia as the source, the dataset benefits from a wide range of topics, sentence structures, and language styles. Moreover, the dataset is balanced in terms of coherent (1) and incoherent (0) labels, ensuring that the model does not develop a bias towards one class.\n",
        "\n",
        "The dataset is available on https://huggingface.co/datasets/dennlinger/wiki-paragraphs and the size is substantially large with over 25 million training pairs and millions more for validation and testing. For the given resources, the dataset is shrunk down to 100,000 sentences with a 70,15,15 split."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Skip the next 3 lines while re running the notebook. The dataset has over 30 million sentence pairs and cannot fit in the drive. Hence it is randomly sampled to 100,000 sentence pairs. This is then saved in the wiki-paragraphs-sampled folder and can be directly loaded from there.**"
      ],
      "metadata": {
        "id": "7qAy33bGFngU"
      },
      "id": "7qAy33bGFngU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "768f1ef2-9545-402b-b182-9b348caeecd3",
      "metadata": {
        "id": "768f1ef2-9545-402b-b182-9b348caeecd3"
      },
      "outputs": [],
      "source": [
        "def explore_wiki_dataset(train_path, val_path, test_path):\n",
        "    \"\"\"\n",
        "    Explore the Wiki Paragraphs dataset from CSV files.\n",
        "\n",
        "    Args:\n",
        "        train_path (str): Path to the training CSV file\n",
        "        val_path (str): Path to the validation CSV file\n",
        "        test_path (str): Path to the test CSV file\n",
        "    \"\"\"\n",
        "    # Load the datasets\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    val_df = pd.read_csv(val_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "\n",
        "    # Display dataset sizes\n",
        "    print(f\"Dataset sizes:\")\n",
        "    print(f\"Train: {len(train_df)} examples\")\n",
        "    print(f\"Validation: {len(val_df)} examples\")\n",
        "    print(f\"Test: {len(test_df)} examples\")\n",
        "\n",
        "    # Display column names\n",
        "    print(f\"\\nColumns in the dataset: {list(train_df.columns)}\")\n",
        "\n",
        "    # Display class distribution\n",
        "    print(f\"\\nClass distribution:\")\n",
        "    print(f\"Train: {train_df['label'].mean():.2f} positive\")\n",
        "    print(f\"Validation: {val_df['label'].mean():.2f} positive\")\n",
        "    print(f\"Test: {test_df['label'].mean():.2f} positive\")\n",
        "\n",
        "    # Display a few examples\n",
        "    print(\"\\nExample from train dataset:\")\n",
        "    sample = train_df.iloc[0]\n",
        "    print(f\"Sentence 1: {sample['sentence1']}\")\n",
        "    print(f\"Sentence 2: {sample['sentence2']}\")\n",
        "    print(f\"Label: {sample['label']}\")\n",
        "\n",
        "    return train_df, val_df, test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "177462bc-c275-454c-b26f-554d88d127d3",
      "metadata": {
        "id": "177462bc-c275-454c-b26f-554d88d127d3",
        "outputId": "a01475fb-3715-4fc0-8fec-5e5214b4339b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset sizes:\n",
            "Train: 25375582 examples\n",
            "Validation: 3163684 examples\n",
            "Test: 3171464 examples\n",
            "\n",
            "Columns in the dataset: ['sentence1', 'sentence2', 'label']\n",
            "\n",
            "Class distribution:\n",
            "Train: 0.50 positive\n",
            "Validation: 0.50 positive\n",
            "Test: 0.50 positive\n",
            "\n",
            "Example from train dataset:\n",
            "Sentence 1: Terminfo:10904051\n",
            "Sentence 2: Terminfo is a library and database that enables programs to use display terminals in a device-independent manner.\n",
            "Label: 1\n"
          ]
        }
      ],
      "source": [
        "train_df, val_df, test_df = explore_wiki_dataset(\n",
        "    train_path=\"wiki-paragraphs/train.csv\",\n",
        "    val_path=\"wiki-paragraphs/validation.csv\",\n",
        "    test_path=\"wiki-paragraphs/test.csv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0e538f8-6069-42d2-9a43-17351f311876",
      "metadata": {
        "id": "b0e538f8-6069-42d2-9a43-17351f311876",
        "outputId": "3f0e942e-6833-42aa-d376-9249b45398b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampling 70000 examples from training set...\n",
            "Sampling 15000 examples from validation set...\n",
            "Sampling 15000 examples from test set...\n",
            "\n",
            "New dataset sizes:\n",
            "Train: 70000 examples\n",
            "Validation: 15000 examples\n",
            "Test: 15000 examples\n",
            "\n",
            "New class distribution:\n",
            "Train: 0.50 positive\n",
            "Validation: 0.50 positive\n",
            "Test: 0.50 positive\n",
            "\n",
            "Saving new splits to CSV files...\n",
            "Saved to wiki-paragraphs-sampled/ directory\n"
          ]
        }
      ],
      "source": [
        "def sample_directly_from_wiki_dataset(train_path, val_path, test_path,\n",
        "                                      train_size=70000, val_size=15000, test_size=15000,\n",
        "                                      random_state=42):\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "        train_path (str): Path to the training CSV file\n",
        "        val_path (str): Path to the validation CSV file\n",
        "        test_path (str): Path to the test CSV file\n",
        "        train_size (int): Number of examples to sample from train set\n",
        "        val_size (int): Number of examples to sample from validation set\n",
        "        test_size (int): Number of examples to sample from test set\n",
        "        random_state (int): Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        tuple: (sampled_train_df, sampled_val_df, sampled_test_df)\n",
        "    \"\"\"\n",
        "    # Make sure the output directory exists\n",
        "    os.makedirs(\"wiki-paragraphs-sampled\", exist_ok=True)\n",
        "\n",
        "    # Sample from training set\n",
        "    print(f\"Sampling {train_size} examples from training set...\")\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    sampled_train = train_df.sample(n=train_size, random_state=random_state)\n",
        "    del train_df  # Free up memory\n",
        "\n",
        "    # Sample from validation set\n",
        "    print(f\"Sampling {val_size} examples from validation set...\")\n",
        "    val_df = pd.read_csv(val_path)\n",
        "    sampled_val = val_df.sample(n=val_size, random_state=random_state)\n",
        "    del val_df  # Free up memory\n",
        "\n",
        "    # Sample from test set\n",
        "    print(f\"Sampling {test_size} examples from test set...\")\n",
        "    test_df = pd.read_csv(test_path)\n",
        "    sampled_test = test_df.sample(n=test_size, random_state=random_state)\n",
        "    del test_df  # Free up memory\n",
        "\n",
        "    # Show the new sizes\n",
        "    print(f\"\\nNew dataset sizes:\")\n",
        "    print(f\"Train: {len(sampled_train)} examples\")\n",
        "    print(f\"Validation: {len(sampled_val)} examples\")\n",
        "    print(f\"Test: {len(sampled_test)} examples\")\n",
        "\n",
        "    # Check class balance\n",
        "    print(f\"\\nNew class distribution:\")\n",
        "    print(f\"Train: {sampled_train['label'].mean():.2f} positive\")\n",
        "    print(f\"Validation: {sampled_val['label'].mean():.2f} positive\")\n",
        "    print(f\"Test: {sampled_test['label'].mean():.2f} positive\")\n",
        "\n",
        "    # Save the new splits to CSV files\n",
        "    print(\"\\nSaving new splits to CSV files...\")\n",
        "    sampled_train.to_csv(\"wiki-paragraphs-sampled/train.csv\", index=False)\n",
        "    sampled_val.to_csv(\"wiki-paragraphs-sampled/validation.csv\", index=False)\n",
        "    sampled_test.to_csv(\"wiki-paragraphs-sampled/test.csv\", index=False)\n",
        "    print(\"Saved to wiki-paragraphs-sampled/ directory\")\n",
        "\n",
        "    return sampled_train, sampled_val, sampled_test\n",
        "\n",
        "# Run the function\n",
        "train_df, val_df, test_df = sample_directly_from_wiki_dataset(\n",
        "    train_path=\"wiki-paragraphs/train.csv\",\n",
        "    val_path=\"wiki-paragraphs/validation.csv\",\n",
        "    test_path=\"wiki-paragraphs/test.csv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebc34bd9-82ba-406c-bdb2-17cd02e652ed",
      "metadata": {
        "id": "ebc34bd9-82ba-406c-bdb2-17cd02e652ed"
      },
      "source": [
        "## Run from this block to directly use the randomly sampled dataset of (100,000 sentence pairs)\n",
        "**Note:** If the samples are already loaded, it can be directly run from here saving time and space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6d02c58-32d4-420f-9ba1-5f589ae73697",
      "metadata": {
        "id": "b6d02c58-32d4-420f-9ba1-5f589ae73697"
      },
      "outputs": [],
      "source": [
        "#Run this directly if dataset is already loaded and sampled\n",
        "train_df = pd.read_csv('/content/drive/My Drive/CS225_final_project/wiki-paragraphs-sampled/train.csv')\n",
        "val_df = pd.read_csv(\"/content/drive/My Drive/CS225_final_project/wiki-paragraphs-sampled/validation.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/My Drive/CS225_final_project/wiki-paragraphs-sampled/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4efaccd5-ac81-4867-8564-e4ddae1a37e4",
      "metadata": {
        "id": "4efaccd5-ac81-4867-8564-e4ddae1a37e4"
      },
      "outputs": [],
      "source": [
        "train_data = train_df.reset_index(drop=True)\n",
        "val_data = val_df.reset_index(drop=True)\n",
        "test_data = test_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1b419da-f8d1-4a68-bb5b-d4ee9e2845f4",
      "metadata": {
        "id": "d1b419da-f8d1-4a68-bb5b-d4ee9e2845f4",
        "outputId": "f7322047-f814-41bc-fde2-055dd82c184d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['sentence1', 'sentence2', 'label'], dtype='object')\n",
            "                                           sentence1  \\\n",
            "0  Damaging agents: Although numerous insects and...   \n",
            "1  Pyatt became an unrestricted free agent after ...   \n",
            "2  Beginning performance 13 January 2012, Dillon ...   \n",
            "3  In 2007, an addition opened, providing a resea...   \n",
            "4  Recovering from a major knee operation, Bennet...   \n",
            "\n",
            "                                           sentence2  label  \n",
            "0  The hickory bark beetle (\"Scolytus quadrispino...      1  \n",
            "1  On May 24, 2016, the Ottawa Senators signed hi...      1  \n",
            "2  Dillon was also the understudy for the charact...      0  \n",
            "3  Helen Bentley donated of land on Bentley Road ...      1  \n",
            "4  While growing up in Nottingham, England, Benne...      0  \n"
          ]
        }
      ],
      "source": [
        "print(train_data.columns)\n",
        "print(train_data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06fd5dd8-d521-4d3a-a115-11748f3d6831",
      "metadata": {
        "id": "06fd5dd8-d521-4d3a-a115-11748f3d6831"
      },
      "source": [
        "## Fine-tuning a BERT-based NSP Model\n",
        "\n",
        "In this step, we fine-tuned a pre-trained BERT model (using the bert-base-uncased architecture) for the Next Sentence Prediction (NSP) task. The training involved using a labeled dataset of sentence pairs where each pair is assigned a label indicating coherence (1) or incoherence (0).\n",
        "\n",
        "Dataset Preparation: The sentence pairs were tokenized and converted into a format suitable for BERT using the Hugging Face BertTokenizer. The input included two sentences concatenated with special tokens and padded or truncated to a fixed length. Labels were added as the target output.\n",
        "\n",
        "Fine-tuning Process: When fine-tuning the BERT model for the NSP task, several parameters and arguments were carefully chosen to balance performance, resource constraints, and ease of implementation.\n",
        "\n",
        "- Batch Size\n",
        "\n",
        "Training batch size: 16\n",
        "\n",
        "Evaluation batch size: 32\n",
        "\n",
        "A smaller batch size (16) was chosen for training to fit the model into memory while processing relatively long sequences. A larger batch size (32) for evaluation allows more efficient use of resources since the model isn’t updating gradients during evaluation, and the focus is on computing metrics.\n",
        "\n",
        "- Learning Rate\n",
        "\n",
        "Value: 2e-5\n",
        "\n",
        "This rate is small enough to allow stable convergence but large enough to make meaningful progress within a limited number of steps. Fine-tuning pre-trained transformer models often benefits from relatively low learning rates because the model’s weights are already initialized to a useful state, and small updates are sufficient to adapt to the new task.\n",
        "\n",
        "- Number of Epochs\n",
        "\n",
        "Value: 1\n",
        "\n",
        "A single epoch was used to prevent overfitting, given that the model already starts from a strong pre-trained base and the training set is large. Running more epochs might lead to marginal gains in accuracy, but it also increases training time and the risk of overfitting, particularly if the dataset is not extremely diverse.\n",
        "\n",
        "- Warmup Steps\n",
        "\n",
        "Value: 100\n",
        "\n",
        "Warmup steps are employed to gradually increase the learning rate at the beginning of training. This helps avoid large, sudden updates to the model’s weights in the initial steps, which could destabilize training. By warming up over 100 steps, the model starts training smoothly and settles into a stable learning pattern.\n",
        "\n",
        "- Weight Decay\n",
        "\n",
        "Value: 0.01\n",
        "\n",
        "Adding a small weight decay term helps regularize the model and prevent overfitting by penalizing large weights. This encourages the model to learn more generalizable features rather than fitting noise in the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d1eba0a-80ec-4bfa-b9ef-af5f8ea14dab",
      "metadata": {
        "id": "6d1eba0a-80ec-4bfa-b9ef-af5f8ea14dab"
      },
      "outputs": [],
      "source": [
        "class SentencePairDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for sentence pairs with next sentence prediction labels.\n",
        "    \"\"\"\n",
        "    def __init__(self, sentence_pairs, tokenizer, max_length=128):\n",
        "        self.sentence_pairs = sentence_pairs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentence_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.sentence_pairs.iloc[idx]\n",
        "        sentence_a = pair['sentence1']\n",
        "        sentence_b = pair['sentence2']\n",
        "        label = pair['label']\n",
        "        # Tokenize the sentence pair\n",
        "        encoding = self.tokenizer(sentence_a, sentence_b,\n",
        "                                  truncation=True,\n",
        "                                  max_length=self.max_length,\n",
        "                                  padding='max_length',\n",
        "                                  return_tensors='pt')\n",
        "\n",
        "        encoding = {key: val.squeeze(0) for key, val in encoding.items()}\n",
        "        encoding['labels'] = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b85d1721-f12b-4d35-9af8-669080890dd5",
      "metadata": {
        "id": "b85d1721-f12b-4d35-9af8-669080890dd5"
      },
      "outputs": [],
      "source": [
        "def train_bert_nsp_model(train_data, val_data, model_name=\"bert-base-uncased\", output_dir=\"./bert_nsp_model\"):\n",
        "    \"\"\"\n",
        "    Fine-tune a BERT model for Next Sentence Prediction.\n",
        "\n",
        "    Args:\n",
        "        train_data (list): List of training sentence pairs\n",
        "        val_data (list): List of validation sentence pairs\n",
        "        model_name (str): Name of the pre-trained BERT model to use\n",
        "        output_dir (str): Directory to save the fine-tuned model\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, tokenizer)\n",
        "    \"\"\"\n",
        "    # Load the tokenizer and model\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertForNextSentencePrediction.from_pretrained(model_name)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = SentencePairDataset(train_data, tokenizer)\n",
        "    val_dataset = SentencePairDataset(val_data, tokenizer)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=250,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        greater_is_better=True,\n",
        "        learning_rate=2e-5,\n",
        "    )\n",
        "\n",
        "    # Define compute_metrics function for evaluation\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        accuracy = accuracy_score(labels, predictions)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1\n",
        "        }\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Training the BERT NSP model...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the best model and tokenizer\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d0f6e26-fd06-40d7-bd83-230e4ceca3ce",
      "metadata": {
        "id": "0d0f6e26-fd06-40d7-bd83-230e4ceca3ce"
      },
      "outputs": [],
      "source": [
        "def evaluate_bert_nsp_model(model, tokenizer, test_data):\n",
        "    \"\"\"\n",
        "    Evaluate the fine-tuned BERT NSP model on test data.\n",
        "\n",
        "    Args:\n",
        "        model: Fine-tuned BERT NSP model\n",
        "        tokenizer: BERT tokenizer\n",
        "        test_data (list): List of test sentence pairs\n",
        "\n",
        "    Returns:\n",
        "        dict: Evaluation metrics\n",
        "    \"\"\"\n",
        "    # Create test dataset\n",
        "    test_dataset = SentencePairDataset(test_data, tokenizer)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    # Move model to device\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Evaluate\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(**{k: v for k, v in batch.items() if k != 'labels'})\n",
        "\n",
        "            # Get predictions\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            labels = batch['labels'].cpu().numpy()\n",
        "\n",
        "            all_preds.extend(predictions)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Test Precision: {precision:.4f}\")\n",
        "    print(f\"Test Recall: {recall:.4f}\")\n",
        "    print(f\"Test F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Return metrics\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9156594c-0cd5-44c7-a35b-22cb2e26444f",
      "metadata": {
        "id": "9156594c-0cd5-44c7-a35b-22cb2e26444f"
      },
      "outputs": [],
      "source": [
        "def predict_sentence_coherence(model, tokenizer, sentence_a, sentence_b):\n",
        "    \"\"\"\n",
        "    Predict whether sentence_b is a coherent continuation of sentence_a.\n",
        "\n",
        "    Args:\n",
        "        model: Fine-tuned BERT NSP model\n",
        "        tokenizer: BERT tokenizer\n",
        "        sentence_a (str): First sentence\n",
        "        sentence_b (str): Second sentence\n",
        "\n",
        "    Returns:\n",
        "        tuple: (is_coherent, confidence_score)\n",
        "    \"\"\"\n",
        "    # Tokenize the sentence pair\n",
        "    encoding = tokenizer(sentence_a, sentence_b,\n",
        "                         truncation=True,\n",
        "                         max_length=128,\n",
        "                         padding='max_length',\n",
        "                         return_tensors='pt')\n",
        "\n",
        "    # Move to device\n",
        "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
        "\n",
        "    # Get prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoding)\n",
        "\n",
        "    # Process outputs\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "    is_next_prob = probabilities[0, 0].item()  # Probability of being \"IsNext\"\n",
        "    is_coherent = is_next_prob >= 0.5\n",
        "\n",
        "    return is_coherent, is_next_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1432592f-5c41-4f1d-ba99-ceb8039d8e0c",
      "metadata": {
        "id": "1432592f-5c41-4f1d-ba99-ceb8039d8e0c"
      },
      "outputs": [],
      "source": [
        "from transformers.utils import logging\n",
        "logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: Skip the next cell if GPU memory is the issue. Training can be slow and heavy. The model has been loaded in the 'bert_nsp_model' folder so that training doesn't need to be repeated. The trained model can then be directly reused from there.**"
      ],
      "metadata": {
        "id": "eBXDFdd6kClu"
      },
      "id": "eBXDFdd6kClu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54af94c0-34f3-46f3-9976-4031c15dab6f",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9ef60ba1bca0458ab624ab7c9e27a8a0"
          ]
        },
        "id": "54af94c0-34f3-46f3-9976-4031c15dab6f",
        "outputId": "9ed27c7f-8aa0-4376-859f-d5e554234a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training the BERT NSP model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.6137, 'learning_rate': 1.929824561403509e-05, 'epoch': 0.06}\n",
            "{'loss': 0.6252, 'learning_rate': 1.8128654970760235e-05, 'epoch': 0.11}\n",
            "{'loss': 0.5994, 'learning_rate': 1.695906432748538e-05, 'epoch': 0.17}\n",
            "{'loss': 0.6086, 'learning_rate': 1.578947368421053e-05, 'epoch': 0.23}\n",
            "{'loss': 0.5841, 'learning_rate': 1.4619883040935675e-05, 'epoch': 0.29}\n",
            "{'loss': 0.5862, 'learning_rate': 1.345029239766082e-05, 'epoch': 0.34}\n",
            "{'loss': 0.5931, 'learning_rate': 1.2280701754385966e-05, 'epoch': 0.4}\n",
            "{'loss': 0.5728, 'learning_rate': 1.1111111111111113e-05, 'epoch': 0.46}\n",
            "{'loss': 0.5765, 'learning_rate': 9.941520467836257e-06, 'epoch': 0.51}\n",
            "{'loss': 0.5806, 'learning_rate': 8.771929824561405e-06, 'epoch': 0.57}\n",
            "{'loss': 0.5725, 'learning_rate': 7.60233918128655e-06, 'epoch': 0.63}\n",
            "{'loss': 0.5685, 'learning_rate': 6.432748538011696e-06, 'epoch': 0.69}\n",
            "{'loss': 0.5597, 'learning_rate': 5.263157894736842e-06, 'epoch': 0.74}\n",
            "{'loss': 0.5657, 'learning_rate': 4.093567251461989e-06, 'epoch': 0.8}\n",
            "{'loss': 0.5709, 'learning_rate': 2.9239766081871347e-06, 'epoch': 0.86}\n",
            "{'loss': 0.5558, 'learning_rate': 1.7543859649122807e-06, 'epoch': 0.91}\n",
            "{'loss': 0.5434, 'learning_rate': 5.847953216374269e-07, 'epoch': 0.97}\n",
            "{'eval_loss': 0.5664578676223755, 'eval_accuracy': 0.705, 'eval_precision': 0.7049813532232285, 'eval_recall': 0.7054511528721844, 'eval_f1': 0.7052161748051429, 'eval_runtime': 132.7951, 'eval_samples_per_second': 112.956, 'eval_steps_per_second': 3.532, 'epoch': 1.0}\n",
            "{'train_runtime': 1710.5386, 'train_samples_per_second': 40.923, 'train_steps_per_second': 2.558, 'train_loss': 0.6373632428850446, 'epoch': 1.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ef60ba1bca0458ab624ab7c9e27a8a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/469 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.7113\n",
            "Test Precision: 0.7124\n",
            "Test Recall: 0.7074\n",
            "Test F1 Score: 0.7099\n",
            "Sentences are incoherent with confidence 0.2728\n"
          ]
        }
      ],
      "source": [
        "bert_nsp_model, bert_tokenizer = train_bert_nsp_model(train_data, val_data)\n",
        "\n",
        "# Uncomment to evaluate the model on test data\n",
        "bert_nsp_metrics = evaluate_bert_nsp_model(bert_nsp_model, bert_tokenizer, test_data)\n",
        "\n",
        "# Example usage\n",
        "sentence_a = \"The cat sat on the mat.\"\n",
        "sentence_b = \"It was purring loudly.\"\n",
        "is_coherent, confidence = predict_sentence_coherence(bert_nsp_model, bert_tokenizer, sentence_a, sentence_b)\n",
        "print(f\"Sentences are {'coherent' if is_coherent else 'incoherent'} with confidence {confidence:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4b5321f-fd4b-4045-b5c8-72569fd8a083",
      "metadata": {
        "id": "f4b5321f-fd4b-4045-b5c8-72569fd8a083"
      },
      "source": [
        "**Results:** The model was evaluated on a test set of sentence pairs. It achieved an accuracy of 71.1%, with a precision of 71.2%, recall of 70.7%, and F1 score of 70.9%. These metrics indicate a solid performance for this initial fine-tuning phase, showing that the model can effectively distinguish between coherent and incoherent sentences.\n",
        "\n",
        "The model is used to test on a real-world example and it correctly identified the pair as incoherent, providing a confidence score of approximately 27.3%. This example demonstrates how the fine-tuned NSP model can be used to help learners understand when two sentences do not form a logically coherent pair.\n",
        "\n",
        "**Evaluation of Result:** The results of this fine-tuning step suggest that the BERT model effectively learned to identify sentence coherence within the training and test datasets. The metrics are promising, especially given the relatively short training duration. However, there may still be room for improvement, such as fine-tuning with additional epochs, experimenting with different learning rates, or using a larger, more diverse dataset. In terms of practical applications, these results indicate that the model can serve as a reliable starting point for providing coherence feedback to language learners."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cdbd5f8-f3fe-4443-b0a7-eea0c3fd0ac1",
      "metadata": {
        "id": "9cdbd5f8-f3fe-4443-b0a7-eea0c3fd0ac1"
      },
      "source": [
        "## Prompting based approach for NSP\n",
        "\n",
        "The idea for this approach is to leverage the model’s pre-trained knowledge without the need for extensive fine-tuning. Here the model is presented with a sentence pair and asked to decide if the second sentence logically follows the first.\n",
        "\n",
        "A pre-trained language model 'google/flan-t5-base', is loaded along with its tokenizer. The model is then moved to GPU (if available) for efficient inference.\n",
        "\n",
        "Two types of prompting are tested here. One is the standard or direct format which states “Determine if the second sentence is a logical continuation of the first sentence. Answer ‘yes’ if it is coherent, or ‘no’ if it is not coherent.” and the other prompting method is a few shot method where the model receives few examples before performing its tasks.\n",
        "\n",
        "Once the prompt is formatted, it is tokenized and fed into the model. The model generates a short text response (e.g., “yes” or “no”), which is then analyzed to determine coherence. Responses are normalized and mapped to binary labels (coherent or incoherent)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8feaae9-dfd1-455b-9b9b-dc67e6de6085",
      "metadata": {
        "id": "c8feaae9-dfd1-455b-9b9b-dc67e6de6085"
      },
      "outputs": [],
      "source": [
        "def setup_prompt_model(model_name=\"google/flan-t5-base\"):\n",
        "    \"\"\"\n",
        "    Set up a pre-trained language model for prompting.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Name of the pre-trained model to use\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, tokenizer)\n",
        "    \"\"\"\n",
        "    # Load the model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    # Move model to device\n",
        "    model.to(device)\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16c8426e-9a83-4a40-a33d-d87cd675baba",
      "metadata": {
        "id": "16c8426e-9a83-4a40-a33d-d87cd675baba"
      },
      "outputs": [],
      "source": [
        "def is_positive_response(response):\n",
        "    \"\"\"\n",
        "    Normalize and check for clear affirmative response.\n",
        "    \"\"\"\n",
        "    normalized = response.strip().lower()\n",
        "    return normalized in {\"yes\", \"yes.\", \"coherent\", \"logical\", \"yes it is\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9583ab2b-6e49-4309-bbbc-82d0d575985c",
      "metadata": {
        "id": "9583ab2b-6e49-4309-bbbc-82d0d575985c"
      },
      "outputs": [],
      "source": [
        "def predict_coherence_with_prompting(model, tokenizer, sentence_a, sentence_b, prompt_template=None):\n",
        "    \"\"\"\n",
        "    Predict whether sentence_b is a coherent continuation of sentence_a using prompting.\n",
        "\n",
        "    Args:\n",
        "        model: Pre-trained language model\n",
        "        tokenizer: Tokenizer for the model\n",
        "        sentence_a (str): First sentence\n",
        "        sentence_b (str): Second sentence\n",
        "        prompt_template (str, optional): Custom prompt template\n",
        "\n",
        "    Returns:\n",
        "        tuple: (is_coherent, raw_output)\n",
        "    \"\"\"\n",
        "    # Default prompt template if none provided\n",
        "    if prompt_template is None:\n",
        "        prompt_template = (\n",
        "            \"Determine if the second sentence is a logical continuation of the first sentence. \"\n",
        "            \"Answer with 'yes' if it is coherent, or 'no' if it is not coherent.\\n\\n\"\n",
        "            \"First sentence: {sentence_a}\\n\"\n",
        "            \"Second sentence: {sentence_b}\\n\\n\"\n",
        "            \"Is the second sentence a coherent continuation of the first? \"\n",
        "        )\n",
        "\n",
        "    # Format the prompt\n",
        "    prompt = prompt_template.format(sentence_a=sentence_a, sentence_b=sentence_b)\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "    # Generate response\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=10,  # Short response expected\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower()\n",
        "    is_coherent = is_positive_response(response)\n",
        "\n",
        "    return is_coherent, response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd9e6820-cbd5-47d7-8069-ea6fdf39973a",
      "metadata": {
        "id": "cd9e6820-cbd5-47d7-8069-ea6fdf39973a"
      },
      "outputs": [],
      "source": [
        "def evaluate_prompt_approach(model, tokenizer, test_data, prompt_template=None):\n",
        "    \"\"\"\n",
        "    Evaluate the prompting-based approach on test data.\n",
        "\n",
        "    Args:\n",
        "        model: Pre-trained language model\n",
        "        tokenizer: Tokenizer for the model\n",
        "        test_data (list): List of test sentence pairs\n",
        "        prompt_template (str, optional): Custom prompt template\n",
        "\n",
        "    Returns:\n",
        "        dict: Evaluation metrics\n",
        "    \"\"\"\n",
        "    # Sample a subset for evaluation (prompting can be slow)\n",
        "    if len(test_data) > 100:\n",
        "        sampled_test_data = random.sample(test_data, 100)\n",
        "    else:\n",
        "        sampled_test_data = test_data\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_responses = []\n",
        "\n",
        "    # Evaluate\n",
        "    for pair in tqdm(sampled_test_data, desc=\"Evaluating prompting approach\"):\n",
        "        sentence_a = pair['sentence_a']\n",
        "        sentence_b = pair['sentence_b']\n",
        "        true_label = pair['label']\n",
        "\n",
        "        # Get prediction\n",
        "        is_coherent, response = predict_coherence_with_prompting(\n",
        "            model, tokenizer, sentence_a, sentence_b, prompt_template\n",
        "        )\n",
        "\n",
        "        # Convert boolean to int (0/1)\n",
        "        pred_label = 1 if is_coherent else 0\n",
        "\n",
        "        all_preds.append(pred_label)\n",
        "        all_labels.append(true_label)\n",
        "        all_responses.append(response)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Prompt Approach Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Prompt Approach Test Precision: {precision:.4f}\")\n",
        "    print(f\"Prompt Approach Test Recall: {recall:.4f}\")\n",
        "    print(f\"Prompt Approach Test F1 Score: {f1:.4f}\")\n",
        "\n",
        "    print(\"\\nExample responses:\")\n",
        "    for i in range(min(5, len(all_responses))):\n",
        "        print(f\"Response {i+1}: {all_responses[i]}\")\n",
        "\n",
        "    # Return metrics\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'responses': all_responses\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a84a266-08b0-42c4-952a-55e1b8ae9491",
      "metadata": {
        "id": "6a84a266-08b0-42c4-952a-55e1b8ae9491"
      },
      "outputs": [],
      "source": [
        "def experiment_with_different_prompts(model, tokenizer, test_data):\n",
        "    \"\"\"\n",
        "    Experiment with different prompt templates and compare their performance.\n",
        "\n",
        "    Args:\n",
        "        model: Pre-trained language model\n",
        "        tokenizer: Tokenizer for the model\n",
        "        test_data (list): List of test sentence pairs\n",
        "\n",
        "    Returns:\n",
        "        dict: Results for each prompt template\n",
        "    \"\"\"\n",
        "    # Define different prompt templates\n",
        "    prompt_templates = {\n",
        "        \"standard\": (\n",
        "            \"Determine if the second sentence is a logical continuation of the first sentence. \"\n",
        "            \"Answer with 'yes' if it is coherent, or 'no' if it is not coherent.\\n\\n\"\n",
        "            \"First sentence: {sentence_a}\\n\"\n",
        "            \"Second sentence: {sentence_b}\\n\\n\"\n",
        "            \"Is the second sentence a coherent continuation of the first? \"\n",
        "        ),\n",
        "\n",
        "        \"few_shot\": (\n",
        "            \"Below are examples of sentence pairs. For each pair, decide if the second sentence \"\n",
        "            \"logically follows the first. Answer \\\"Yes\\\" if it does, or \\\"No\\\" if it does not.\\n\\n\"\n",
        "            \"Example 1:\\n\"\n",
        "            \"First: The dog chased the ball into the yard.\\n\"\n",
        "            \"Second: It brought the ball back to its owner.\\n\"\n",
        "            \"Answer: Yes\\n\\n\"\n",
        "            \"Example 2:\\n\"\n",
        "            \"First: The sun set behind the mountains.\\n\"\n",
        "            \"Second: I finished my homework before breakfast.\\n\"\n",
        "            \"Answer: No\\n\\n\"\n",
        "            \"Example 3:\\n\"\n",
        "            \"First: The cake was ready to serve.\\n\"\n",
        "            \"Second: Everyone sat down at the table to eat dessert.\\n\"\n",
        "            \"Answer: Yes\\n\\n\"\n",
        "            \"Now, consider this pair:\\n\"\n",
        "            \"First: {sentence_a}\\n\"\n",
        "            \"Second: {sentence_b}\\n\"\n",
        "            \"Answer: \"\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # Sample a small subset for quick experimentation\n",
        "    if len(test_data) > 100:\n",
        "        sampled_test_data = random.sample(test_data, 100)\n",
        "    else:\n",
        "        sampled_test_data = test_data\n",
        "\n",
        "    # Evaluate each prompt template\n",
        "    results = {}\n",
        "    for name, template in prompt_templates.items():\n",
        "        print(f\"\\nEvaluating prompt template: {name}\")\n",
        "        metrics = evaluate_prompt_approach(model, tokenizer, sampled_test_data, template)\n",
        "        results[name] = metrics\n",
        "\n",
        "    # Compare results\n",
        "    print(\"\\nPrompt Template Comparison:\")\n",
        "    for name, metrics in results.items():\n",
        "        print(f\"{name}: Accuracy={metrics['accuracy']:.4f}, F1={metrics['f1']:.4f}\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1004612c-c11d-462c-ab37-13a519a02266",
      "metadata": {
        "id": "1004612c-c11d-462c-ab37-13a519a02266"
      },
      "outputs": [],
      "source": [
        "test_data = test_data.rename(columns={\n",
        "    'sentence1': 'sentence_a',\n",
        "    'sentence2': 'sentence_b'\n",
        "}).to_dict(orient='records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca376669-1cac-423c-ba0c-e66e541a6ade",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "cc19c1a21b0541b48294e2ef3d1128a6",
            "ab604b1ac4184a6ab5b9a3b5919f3b7f"
          ]
        },
        "id": "ca376669-1cac-423c-ba0c-e66e541a6ade",
        "outputId": "168a8c69-75fc-4020-ad7f-0078ebc9fc72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating prompt template: standard\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc19c1a21b0541b48294e2ef3d1128a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating prompting approach:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt Approach Test Accuracy: 0.5100\n",
            "Prompt Approach Test Precision: 0.4878\n",
            "Prompt Approach Test Recall: 0.4167\n",
            "Prompt Approach Test F1 Score: 0.4494\n",
            "\n",
            "Example responses:\n",
            "Response 1: yes\n",
            "Response 2: no\n",
            "Response 3: no\n",
            "Response 4: no\n",
            "Response 5: yes\n",
            "\n",
            "Evaluating prompt template: few_shot\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab604b1ac4184a6ab5b9a3b5919f3b7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating prompting approach:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt Approach Test Accuracy: 0.5600\n",
            "Prompt Approach Test Precision: 0.5476\n",
            "Prompt Approach Test Recall: 0.4792\n",
            "Prompt Approach Test F1 Score: 0.5111\n",
            "\n",
            "Example responses:\n",
            "Response 1: yes\n",
            "Response 2: no\n",
            "Response 3: no\n",
            "Response 4: no\n",
            "Response 5: answer: yes\n",
            "\n",
            "Prompt Template Comparison:\n",
            "standard: Accuracy=0.5100, F1=0.4494\n",
            "few_shot: Accuracy=0.5600, F1=0.5111\n",
            "Sentences are incoherent\n",
            "Model response: no\n"
          ]
        }
      ],
      "source": [
        "prompt_model, prompt_tokenizer = setup_prompt_model()\n",
        "\n",
        "#prompt_metrics = evaluate_prompt_approach(prompt_model, prompt_tokenizer, test_data)\n",
        "\n",
        "prompt_experiment_results = experiment_with_different_prompts(prompt_model, prompt_tokenizer, test_data)\n",
        "\n",
        "# Example usage\n",
        "sentence_a = \"The cat sat on the mat.\"\n",
        "sentence_b = \"It was purring loudly.\"\n",
        "is_coherent, response = predict_coherence_with_prompting(prompt_model, prompt_tokenizer, sentence_a, sentence_b)\n",
        "print(f\"Sentences are {'coherent' if is_coherent else 'incoherent'}\")\n",
        "print(f\"Model response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8616479-ea48-4a06-93f4-db819f46bd36",
      "metadata": {
        "id": "f8616479-ea48-4a06-93f4-db819f46bd36"
      },
      "source": [
        "#### Results\n",
        "The prompting based model performance is tested on the test set extracted from the wiki-paragraphs dataset.\n",
        "\n",
        "Using the standard prompt, the approach achieved an accuracy of 51% and an F1 score of 0.4494.\n",
        "\n",
        "Few-shot prompting, on the other hand, yielded better results, with an accuracy of 56% and an F1 score of 0.5111.\n",
        "\n",
        "The results demonstrate that both the standard prompting and few-shot prompting approaches are capable of predicting sentence coherence, though the overall accuracy and F1 scores leave room for improvement. It also suggests that including examples in the prompt helps the model make more informed decisions, likely by clarifying the task and setting clearer expectations.\n",
        "\n",
        "While prompting leverages a pre-trained model’s existing knowledge, it may not fully adapt to the nuances of the NSP task. Therefore fine-tuning on a labeled dataset and then prompting can result in better accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b659da-3669-4da4-86b9-f9b33f463870",
      "metadata": {
        "id": "70b659da-3669-4da4-86b9-f9b33f463870"
      },
      "source": [
        "## Retrieval-Augmented Generation Based NSP Approach\n",
        "\n",
        "In this section, a retrieval-augmented generation (RAG) methodology is used for coherence detection. The key idea behind RAG is to leverage a large collection of text and build a searchable index that allows retrieval of relevant context for any given input sentence. This approach enables the model to incorporate more meaningful background information into its predictions, rather than solely relying on the sentence pairs provided.\n",
        "\n",
        "#### Data Preparation:\n",
        "As the objective is to teach young children or non-English speakers, no difficult vocabulary is used. Text from Wikipedia is extracted using an online Wiki text extractor and then inputted below as a text file. The next step involved reading raw text files containing various paragraphs or articles and splitting them into sentences. These sentences were then cleaned, normalized, and filtered to remove very short or incomplete lines. Once prepared, the sentences served as the foundation for building the index and generating sentence pairs.\n",
        "\n",
        "#### Sentence Pair Creation:\n",
        "From the pool of sentences, coherent (positive) pairs were formed by taking consecutive sentences, while incoherent (negative) pairs were constructed by randomly selecting non-consecutive sentences. This approach ensured a balanced dataset with a mix of logically connected and unrelated sentence pairs, which is crucial for training and evaluating the coherence detection model.\n",
        "\n",
        "#### Building a FAISS Index:\n",
        "Using a pre-trained sentence encoder, all sentences were transformed into high-dimensional vector embeddings. To improve similarity-based retrieval, these embeddings were normalized for cosine similarity. A FAISS index was then built, which allows for fast and efficient similarity searches. The index enables us to retrieve context sentences that are most relevant to any input sentence, providing additional information that can improve the accuracy of coherence judgments.\n",
        "\n",
        "#### Contextual Predictions with RAG:\n",
        "For each test sentence pair, the FAISS index was queried to retrieve several related sentences from the corpus. These retrieved sentences formed a context block that was appended to the first sentence before comparing it with the second sentence. The model, using this enriched input, then predicted whether the pair was coherent or incoherent. By augmenting the input with relevant background context, the model was better equipped to make informed predictions.\n",
        "\n",
        "#### Evaluation and Results:\n",
        "The RAG-based approach was evaluated on a limited set of samples, as the retrieval process and context enhancement can be computationally intensive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "486c4e27-8f40-4d03-9d1f-d775b33285c7",
      "metadata": {
        "id": "486c4e27-8f40-4d03-9d1f-d775b33285c7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bb3fdf5-240a-48e2-8121-ad707587e312",
      "metadata": {
        "id": "5bb3fdf5-240a-48e2-8121-ad707587e312"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def read_text_file(file_path):\n",
        "    \"\"\"Read text from file and split into sentences\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Clean the content\n",
        "    content = re.sub(r'\\n+', ' ', content)  # Replace multiple newlines with space\n",
        "    content = re.sub(r'\\s+', ' ', content)  # Replace multiple spaces with single space\n",
        "\n",
        "    # Split content into sentences\n",
        "    sentences = sent_tokenize(content)\n",
        "\n",
        "    # Remove very short sentences\n",
        "    sentences = [s for s in sentences if len(s.split()) > 3]\n",
        "\n",
        "    print(f\"Extracted {len(sentences)} sentences from {file_path}\")\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "043ea5fa-4a90-4fab-9e65-1bff577844b5",
      "metadata": {
        "id": "043ea5fa-4a90-4fab-9e65-1bff577844b5"
      },
      "outputs": [],
      "source": [
        "# Function to create sentence pairs for training\n",
        "def create_sentence_pairs(sentences, positive_ratio=0.5):\n",
        "    \"\"\"Create coherent and incoherent sentence pairs\"\"\"\n",
        "    pairs = []\n",
        "    labels = []\n",
        "\n",
        "    # Create positive pairs (coherent sentences)\n",
        "    for i in range(len(sentences) - 1):\n",
        "        pairs.append((sentences[i], sentences[i + 1]))\n",
        "        labels.append(1)  # Coherent pair\n",
        "\n",
        "    # Create negative pairs (incoherent sentences)\n",
        "    num_positive = len(pairs)\n",
        "    num_negative = int(num_positive * (1 - positive_ratio) / positive_ratio)\n",
        "\n",
        "    for _ in range(num_negative):\n",
        "        idx1 = random.randint(0, len(sentences) - 1)\n",
        "        idx2 = random.randint(0, len(sentences) - 1)\n",
        "\n",
        "        # Ensure sentences are not consecutive\n",
        "        while abs(idx1 - idx2) <= 1 or idx1 >= len(sentences) - 1 or idx2 >= len(sentences) - 1:\n",
        "            idx1 = random.randint(0, len(sentences) - 1)\n",
        "            idx2 = random.randint(0, len(sentences) - 1)\n",
        "\n",
        "        pairs.append((sentences[idx1], sentences[idx2]))\n",
        "        labels.append(0)  # Incoherent pair\n",
        "\n",
        "    return pairs, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "006f2e83-6548-405a-a232-6fbea113e0b5",
      "metadata": {
        "id": "006f2e83-6548-405a-a232-6fbea113e0b5"
      },
      "outputs": [],
      "source": [
        "# Function to build FAISS index\n",
        "def build_faiss_index(sentences, sentence_encoder):\n",
        "    \"\"\"Build a FAISS index from a list of sentences\"\"\"\n",
        "    # Remove duplicates while preserving order\n",
        "    unique_sentences = []\n",
        "    seen = set()\n",
        "    for s in sentences:\n",
        "        if s not in seen:\n",
        "            seen.add(s)\n",
        "            unique_sentences.append(s)\n",
        "\n",
        "    # Encode sentences\n",
        "    print(f\"Encoding {len(unique_sentences)} sentences for FAISS index...\")\n",
        "    embeddings = sentence_encoder.encode(unique_sentences, show_progress_bar=True)\n",
        "\n",
        "    # Normalize embeddings for cosine similarity\n",
        "    faiss.normalize_L2(embeddings)\n",
        "\n",
        "    # Build FAISS index\n",
        "    vector_dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(vector_dimension)  # Inner product for cosine similarity\n",
        "    index.add(embeddings)\n",
        "\n",
        "    print(f\"FAISS index built with {index.ntotal} vectors of dimension {vector_dimension}\")\n",
        "    return index, unique_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7854933-ecd9-46ed-bc01-f473b67dec25",
      "metadata": {
        "id": "c7854933-ecd9-46ed-bc01-f473b67dec25"
      },
      "outputs": [],
      "source": [
        "# Function to retrieve context from FAISS index\n",
        "def retrieve_context(query_sentence, faiss_index, sentences_list, sentence_encoder, k=3):\n",
        "    \"\"\"Retrieve context sentences from the FAISS index\"\"\"\n",
        "    # Encode query sentence\n",
        "    query_embedding = sentence_encoder.encode([query_sentence])\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "\n",
        "    # Search the index\n",
        "    distances, indices = faiss_index.search(query_embedding, k+1)  # +1 to account for the query itself\n",
        "\n",
        "    # Get the retrieved sentences (excluding the query if it's in the index)\n",
        "    retrieved_sentences = []\n",
        "    for idx in indices[0]:\n",
        "        if idx < len(sentences_list) and sentences_list[idx] != query_sentence:\n",
        "            retrieved_sentences.append(sentences_list[idx])\n",
        "            if len(retrieved_sentences) >= k:\n",
        "                break\n",
        "\n",
        "    return retrieved_sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76f8548b-d6ff-4fd6-bcd2-c01ad874c03f",
      "metadata": {
        "id": "76f8548b-d6ff-4fd6-bcd2-c01ad874c03f"
      },
      "outputs": [],
      "source": [
        "# Function to predict with context\n",
        "def predict_with_context(sentence_a, sentence_b, faiss_index, sentences_list,\n",
        "                         sentence_encoder, tokenizer, nsp_model, device, retrieval_k=3):\n",
        "    \"\"\"Predict if sentence_b is a coherent continuation of sentence_a using RAG\"\"\"\n",
        "    # Retrieve context from FAISS index\n",
        "    context_sentences = retrieve_context(sentence_a, faiss_index, sentences_list,\n",
        "                                         sentence_encoder, k=retrieval_k)\n",
        "\n",
        "    # Build enhanced input with context\n",
        "    enhanced_context = \" \".join(context_sentences) + \" \" + sentence_a\n",
        "\n",
        "    # Truncate if too long (BERT has a token limit)\n",
        "    if len(enhanced_context.split()) > 250:  # Arbitrary limit to prevent exceeding BERT's token limit\n",
        "        enhanced_context = \" \".join(enhanced_context.split()[-250:])\n",
        "\n",
        "    # Prepare inputs for the NSP model\n",
        "    encoding = tokenizer(enhanced_context, sentence_b, return_tensors='pt')\n",
        "\n",
        "    # can be uncommented if using with the fine tuned Bert based NSP model for max length=512\n",
        "    # encoding = tokenizer(\n",
        "    #     enhanced_context,\n",
        "    #     sentence_b,\n",
        "    #     return_tensors=\"pt\",\n",
        "    #     truncation=True,\n",
        "    #     max_length=512,\n",
        "    #     padding=\"max_length\"\n",
        "    # )\n",
        "\n",
        "    # Move inputs to the appropriate device\n",
        "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
        "\n",
        "    # Get model prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = nsp_model(**encoding)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        prediction = torch.argmax(probs, dim=1).item()\n",
        "        confidence = probs[0][prediction].item()\n",
        "\n",
        "    # Return 1 for IsNextSentence, 0 for NotNextSentence\n",
        "    # BERT's NSP task uses 0 for IsNextSentence and 1 for NotNextSentence, so we invert it\n",
        "    return 1 - prediction, confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c57d463-2ff6-4ed0-8fb1-9ffa877013e5",
      "metadata": {
        "id": "4c57d463-2ff6-4ed0-8fb1-9ffa877013e5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Function to evaluate the model on a limited number of samples\n",
        "def evaluate_samples(test_pairs, test_labels, faiss_index, sentences_list,\n",
        "                            sentence_encoder, tokenizer, nsp_model, device, num_samples=50):\n",
        "    \"\"\"Evaluate the model on a limited number of test samples\"\"\"\n",
        "    # Ensure we have a balanced set of samples (equal positive and negative examples)\n",
        "    pos_pairs = [(pair, label) for pair, label in zip(test_pairs, test_labels) if label == 1]\n",
        "    neg_pairs = [(pair, label) for pair, label in zip(test_pairs, test_labels) if label == 0]\n",
        "\n",
        "    # Take up to half of samples from each class\n",
        "    samples_per_class = min(num_samples // 2, len(pos_pairs), len(neg_pairs))\n",
        "\n",
        "    # If one class has fewer than half the samples, take more from the other class\n",
        "    pos_samples = min(samples_per_class, len(pos_pairs))\n",
        "    neg_samples = min(num_samples - pos_samples, len(neg_pairs))\n",
        "\n",
        "    # If still not enough samples, adjust positive samples again\n",
        "    if pos_samples + neg_samples < num_samples and len(pos_pairs) > pos_samples:\n",
        "        pos_samples = min(num_samples - neg_samples, len(pos_pairs))\n",
        "\n",
        "    # Randomly sample from each class\n",
        "    random.seed(42)  # For reproducibility\n",
        "    sampled_pos = random.sample(pos_pairs, pos_samples)\n",
        "    sampled_neg = random.sample(neg_pairs, neg_samples)\n",
        "\n",
        "    # Combine and shuffle\n",
        "    sampled_pairs = sampled_pos + sampled_neg\n",
        "    random.shuffle(sampled_pairs)\n",
        "\n",
        "    sample_test_pairs = [pair for pair, _ in sampled_pairs]\n",
        "    sample_test_labels = [label for _, label in sampled_pairs]\n",
        "\n",
        "    print(f\"Evaluating on {len(sample_test_pairs)} samples ({pos_samples} coherent, {neg_samples} incoherent)\")\n",
        "\n",
        "    # Evaluate the samples\n",
        "    predictions = []\n",
        "    confidences = []\n",
        "\n",
        "    for i, (sentence_a, sentence_b) in enumerate(sample_test_pairs):\n",
        "        pred, conf = predict_with_context(sentence_a, sentence_b, faiss_index, sentences_list,\n",
        "                                         sentence_encoder, tokenizer, nsp_model, device)\n",
        "        predictions.append(pred)\n",
        "        confidences.append(conf)\n",
        "\n",
        "        # Print progress every 5 pairs\n",
        "        if (i+1) % 5 == 0:\n",
        "            print(f\"Evaluated {i+1}/{len(sample_test_pairs)} pairs...\")\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(sample_test_labels, predictions, target_names=['Incoherent', 'Coherent'])\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = sum(1 for p, t in zip(predictions, sample_test_labels) if p == t) / len(sample_test_labels)\n",
        "\n",
        "    return {\n",
        "        'predictions': predictions,\n",
        "        'confidences': confidences,\n",
        "        'accuracy': accuracy,\n",
        "        'classification_report': report,\n",
        "        'test_pairs': sample_test_pairs,\n",
        "        'test_labels': sample_test_labels\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c9ab3d2-cb09-4e0b-bdc2-102b74274e26",
      "metadata": {
        "id": "0c9ab3d2-cb09-4e0b-bdc2-102b74274e26",
        "outputId": "9fad5daa-d72e-47b6-faea-bc09ace89198"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForNextSentencePrediction(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyNSPHead(\n",
              "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize models\n",
        "sentence_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "nsp_model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "nsp_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba3e45e1-3ae6-4919-8aad-631894224457",
      "metadata": {
        "id": "ba3e45e1-3ae6-4919-8aad-631894224457",
        "outputId": "3a01e28f-5088-4a6f-f376-43da315beba3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 346 sentences from rag-based-nsp/dog.txt\n",
            "Extracted 100 sentences from rag-based-nsp/siamese_cat.txt\n",
            "Total training sentences: 446\n",
            "Created 890 sentence pairs for training\n"
          ]
        }
      ],
      "source": [
        "training_files = [\"/content/drive/My Drive/CS225_final_project/rag-based-nsp/dog.txt\", \"/content/drive/My Drive/CS225_final_project/rag-based-nsp/siamese_cat.txt\"]\n",
        "# Read and process all training files\n",
        "all_training_sentences = []\n",
        "for file_path in training_files:\n",
        "    sentences = read_text_file(file_path)\n",
        "    all_training_sentences.extend(sentences)\n",
        "\n",
        "print(f\"Total training sentences: {len(all_training_sentences)}\")\n",
        "\n",
        "# Create sentence pairs for training\n",
        "training_pairs, training_labels = create_sentence_pairs(all_training_sentences)\n",
        "print(f\"Created {len(training_pairs)} sentence pairs for training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a31ac239-2b60-4b8b-bb55-a30f1c07c6e8",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "834817e40dc740279a466f317f93d961"
          ]
        },
        "id": "a31ac239-2b60-4b8b-bb55-a30f1c07c6e8",
        "outputId": "ef4b3cdd-9f4b-4207-e765-ebbe5c07befb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding 446 sentences for FAISS index...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "834817e40dc740279a466f317f93d961",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS index built with 446 vectors of dimension 384\n"
          ]
        }
      ],
      "source": [
        "# Build FAISS index\n",
        "faiss_index, sentences_list = build_faiss_index(all_training_sentences, sentence_encoder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4117f9bc-f0cb-4627-8f2a-88a84fc5c88a",
      "metadata": {
        "id": "4117f9bc-f0cb-4627-8f2a-88a84fc5c88a",
        "outputId": "60839ed4-a84c-49b8-c01c-07149cd4e529"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing a custom sentence pair:\n",
            "Sentence A: Dogs are known for their loyalty and companionship.\n",
            "Sentence B: Many different breeds have been developed for various purposes.\n",
            "Prediction: Coherent (confidence: 1.0000)\n",
            "\n",
            "Retrieved context:\n",
            "1. These sophisticated forms of social cognition and communication may account for dogs' trainability, playfulness, and ability to fit into human households and social situations, and probably also their co-existence with early human hunter-gatherers.\n",
            "2. Health benefits The scientific evidence is mixed as to whether a dog's companionship can enhance human physical and psychological well-being.\n",
            "3. Cultural importance Artworks have depicted dogs as symbols of guidance, protection, loyalty, fidelity, faithfulness, alertness, and love.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1, 0.9999909400939941)"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def test_single_pair(sentence_a, sentence_b):\n",
        "    \"\"\"Test a single pair of sentences\"\"\"\n",
        "    pred, conf = predict_with_context(sentence_a, sentence_b, faiss_index, sentences_list,\n",
        "                                     sentence_encoder, tokenizer, nsp_model, device)\n",
        "\n",
        "    print(f\"Sentence A: {sentence_a}\")\n",
        "    print(f\"Sentence B: {sentence_b}\")\n",
        "    print(f\"Prediction: {'Coherent' if pred == 1 else 'Incoherent'} (confidence: {conf:.4f})\")\n",
        "\n",
        "    # Show retrieved context\n",
        "    context = retrieve_context(sentence_a, faiss_index, sentences_list, sentence_encoder)\n",
        "    print(\"\\nRetrieved context:\")\n",
        "    for i, ctx in enumerate(context):\n",
        "        print(f\"{i+1}. {ctx}\")\n",
        "\n",
        "    return pred, conf\n",
        "\n",
        "# Example usage of test_single_pair function\n",
        "print(\"\\nTesting a custom sentence pair:\")\n",
        "# Replace with your own sentences to test\n",
        "custom_a = \"Dogs are known for their loyalty and companionship.\"\n",
        "custom_b = \"Many different breeds have been developed for various purposes.\"\n",
        "test_single_pair(custom_a, custom_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b6e0a53-d794-4cac-ad5d-1f8b016c6646",
      "metadata": {
        "id": "9b6e0a53-d794-4cac-ad5d-1f8b016c6646",
        "outputId": "b6b29159-d0fb-4916-a68e-2cbe3ee4ee75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 337 sentences from rag-based-nsp/cat.txt\n",
            "Created 672 sentence pairs for testing\n",
            "\n",
            "Evaluating the model on 50 samples...\n",
            "Evaluating on 50 samples (25 coherent, 25 incoherent)\n",
            "Evaluated 5/50 pairs...\n",
            "Evaluated 10/50 pairs...\n",
            "Evaluated 15/50 pairs...\n",
            "Evaluated 20/50 pairs...\n",
            "Evaluated 25/50 pairs...\n",
            "Evaluated 30/50 pairs...\n",
            "Evaluated 35/50 pairs...\n",
            "Evaluated 40/50 pairs...\n",
            "Evaluated 45/50 pairs...\n",
            "Evaluated 50/50 pairs...\n",
            "\n",
            "Results:\n",
            "Accuracy: 0.5600\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Incoherent       1.00      0.12      0.21        25\n",
            "    Coherent       0.53      1.00      0.69        25\n",
            "\n",
            "    accuracy                           0.56        50\n",
            "   macro avg       0.77      0.56      0.45        50\n",
            "weighted avg       0.77      0.56      0.45        50\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Path to test file\n",
        "test_file = \"/content/drive/My Drive/add-your-path\"\n",
        "\n",
        "# Read and process test file\n",
        "test_sentences = read_text_file(test_file)\n",
        "test_pairs, test_labels = create_sentence_pairs(test_sentences)\n",
        "print(f\"Created {len(test_pairs)} sentence pairs for testing\")\n",
        "\n",
        "# Evaluate the model on limited samples\n",
        "print(\"\\nEvaluating the model on 50 samples...\")\n",
        "results = evaluate_samples(test_pairs, test_labels, faiss_index, sentences_list,\n",
        "                                 sentence_encoder, tokenizer, nsp_model, device, num_samples=50)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nResults:\")\n",
        "print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(results['classification_report'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38a4f3f2-2a32-4d92-b1c9-b7deab4e8763",
      "metadata": {
        "id": "38a4f3f2-2a32-4d92-b1c9-b7deab4e8763"
      },
      "source": [
        "#### Results\n",
        "The overall accuracy of 56% on the 50-sample test set highlights that the RAG-based approach is making progress in coherence detection but is far from perfect. The F1 score for the coherent class is substantially higher than that for the incoherent class. This imbalance suggests that the model is more confident and consistent in identifying coherence, but struggles with nuanced or subtly incoherent pairs. Such a disparity might point to a need for more diverse or challenging examples in the training data, particularly for the incoherent category.\n",
        "\n",
        "The ability to retrieve context from the FAISS index does improve the model’s understanding, as evidenced by the confident prediction on coherent pairs and the clear rationale behind certain decisions. However, the retrieval process is only as good as the data it draws from. As the context is mostly from 'dogs' page and the evaluation is on a 'cats' page it fails to achieve high accuracy. One random example is taken for a dog's case and the prediction obtained was coherent (label 1) with a confidence of 99.999%. This further confirms that the mismatch in context and evaluation text resulted in the lower accuracy. This can be improved by updating and enhancing the size of the context with more relevant information."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4de71334-0c8b-4bcb-a643-76259c2bf102",
      "metadata": {
        "id": "4de71334-0c8b-4bcb-a643-76259c2bf102"
      },
      "source": [
        "## Enhanced NSP/Sentence Coherence Using Tiered Inference Approach\n",
        "\n",
        "This section is the final phase of the sentence coherence prediction project, where the previously tested strategies: fine-tuned Next Sentence Prediction (NSP), Retrieval-Augmented Generation (RAG), and prompting, are integrated into a robust, interpretable, and fallback-safe system.\n",
        "\n",
        "Combining these methods into a unified model pipeline is not practical because:\n",
        "\n",
        "- The fine-tuned BERT NSP model operates as a classification head trained on binary coherence labels.\n",
        "\n",
        "- RAG-based methods rely on external context retrieval and are sensitive to token limits and sequence padding.\n",
        "\n",
        "- Few-shot prompting models like FLAN-T5 are decoder-based and follow instruction templates rather than logits from a classification head.\n",
        "\n",
        "Therefore, the final prediction architecture is a tiered decision approach based on confidence score.\n",
        "\n",
        "\n",
        "The objective is to reuse the fine-tuned BERT NSP model from the first step. If the confidence level of the prediction is below 0.8, the model should fall back to the RAG based prediction. As the test samples are the wiki paragraphs, it is hard to provide a definitive context. Therefore, RAG produces the lowest accuracy in terms of results compared to the other two methods. Hence, it has been commented out in this architecture. If the fine-tuned NSP model isn't confident in its prediction, the model switches to few shot prompting based NSP approach. This time the model chosen is flan-t5-large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "947f7c87-6025-4ac7-9f48-bc6bf1471172",
      "metadata": {
        "id": "947f7c87-6025-4ac7-9f48-bc6bf1471172"
      },
      "outputs": [],
      "source": [
        "# Reload the fine‑tuned BERT NSP checkpoint\n",
        "nsp_model     = BertForNextSentencePrediction.from_pretrained(\"/content/drive/My Drive/CS225_final_project/bert_nsp_model\").to(device)\n",
        "nsp_tokenizer = BertTokenizer.from_pretrained(\"/content/drive/My Drive/CS225_final_project/bert_nsp_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e5c6599-a3ab-4ffd-a674-2a72007d7090",
      "metadata": {
        "id": "0e5c6599-a3ab-4ffd-a674-2a72007d7090",
        "outputId": "621f4599-6494-4558-d7f9-bce0636d1ad5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 1024)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 1024)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 16)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-23): 23 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 1024)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 16)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-23): 23 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Load the flan-t5-large model\n",
        "prompt_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "prompt_model     = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
        "\n",
        "prompt_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56d526e3-aac3-4f0f-842b-e0a0c6218bee",
      "metadata": {
        "id": "56d526e3-aac3-4f0f-842b-e0a0c6218bee"
      },
      "outputs": [],
      "source": [
        "# FEW‑SHOT prompt block\n",
        "\n",
        "FEW_SHOT_TEMPLATE = (\n",
        "    \"Below are examples of sentence pairs. For each pair, decide if the second sentence \"\n",
        "    \"logically follows the first. Answer \\\"Yes\\\" if it does, or \\\"No\\\" if it does not.\\n\\n\"\n",
        "    \"Example 1:\\n\"\n",
        "    \"First: The dog chased the ball into the yard.\\n\"\n",
        "    \"Second: It brought the ball back to its owner.\\n\"\n",
        "    \"Answer: Yes\\n\\n\"\n",
        "    \"Example 2:\\n\"\n",
        "    \"First: The sun set behind the mountains.\\n\"\n",
        "    \"Second: I finished my homework before breakfast.\\n\"\n",
        "    \"Answer: No\\n\\n\"\n",
        "    \"Example 3:\\n\"\n",
        "    \"First: The cake was ready to serve.\\n\"\n",
        "    \"Second: Everyone sat down at the table to eat dessert.\\n\"\n",
        "    \"Answer: Yes\\n\\n\"\n",
        "    \"Now, consider this pair:\\n\"\n",
        "    \"First: {sentence_a}\\n\"\n",
        "    \"Second: {sentence_b}\\n\"\n",
        "    \"Answer: \"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2e68a7d-c174-4f15-89f9-25ac10686867",
      "metadata": {
        "id": "f2e68a7d-c174-4f15-89f9-25ac10686867"
      },
      "outputs": [],
      "source": [
        "def predict_coherence_with_prompting(model, tokenizer, sentence_a, sentence_b, prompt_template):\n",
        "    prompt = prompt_template.format(sentence_a=sentence_a, sentence_b=sentence_b)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=1,      # one‑token answer\n",
        "        num_beams=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    response = tokenizer.decode(output_ids[0],skip_special_tokens=True).strip().lower()\n",
        "\n",
        "    return is_positive_response(response), response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e27251f-fcb2-4c0d-b8d3-7baf5134a5fb",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "8e66e99c7e8749e68ce7b53e03747eba"
          ]
        },
        "id": "4e27251f-fcb2-4c0d-b8d3-7baf5134a5fb",
        "outputId": "c030ded4-956e-4b63-fdfa-3342f6638333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding 138628 sentences for FAISS index...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e66e99c7e8749e68ce7b53e03747eba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/4333 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS index built with 138628 vectors of dimension 384\n"
          ]
        }
      ],
      "source": [
        "# Build a FAISS index on ALL training sentences\n",
        "\n",
        "all_train_sentences = pd.concat([train_df[\"sentence1\"], train_df[\"sentence2\"]]).tolist()\n",
        "sentence_encoder    = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "faiss_index, sentences_list = build_faiss_index(all_train_sentences, sentence_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ddca078-43dc-487d-b912-852c04d8f40f",
      "metadata": {
        "id": "8ddca078-43dc-487d-b912-852c04d8f40f"
      },
      "outputs": [],
      "source": [
        "def combined_predict(sent_a, sent_b, retrieval_k = 2):\n",
        "    \"\"\"\n",
        "    Returns (prediction, chosen_tier, nsp_conf, rag_conf, prompt_conf)\n",
        "    tier = 'NSP' | 'RAG' | 'PROMPT'\n",
        "    \"\"\"\n",
        "    # fine-tuned BERT based NSP\n",
        "    is_coh, p_next = predict_sentence_coherence(\n",
        "        nsp_model, nsp_tokenizer, sent_a, sent_b\n",
        "    )\n",
        "\n",
        "    is_coh = not is_coh\n",
        "    p_next = 1.0 - p_next\n",
        "\n",
        "    nsp_conf = p_next if is_coh else (1.0 - p_next)\n",
        "\n",
        "\n",
        "    if nsp_conf >= 0.8:\n",
        "        return int(is_coh), \"Fine-tuned NSP\", nsp_conf\n",
        "\n",
        "    # RAG‑BERT\n",
        "    # rag_pred, rag_conf = predict_with_context(sent_a, sent_b,faiss_index, sentences_list, sentence_encoder,\n",
        "    #     nsp_tokenizer, nsp_model, device,retrieval_k=retrieval_k)\n",
        "    # if rag_conf >= 0.8:\n",
        "    #     return rag_pred, \"RAG\", rag_conf\n",
        "\n",
        "    # Few‑shot prompting (FLAN‑T5‑Large)\n",
        "    prompt_pred, raw_resp = predict_coherence_with_prompting(\n",
        "        prompt_model, prompt_tokenizer,\n",
        "        sent_a, sent_b,\n",
        "        prompt_template=FEW_SHOT_TEMPLATE\n",
        "    )\n",
        "\n",
        "    return int(prompt_pred), \"PROMPT\", 0.8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e6f7461-a56f-40aa-9a38-0c488a5db9ee",
      "metadata": {
        "id": "8e6f7461-a56f-40aa-9a38-0c488a5db9ee",
        "outputId": "93d22a2e-d0c2-47f1-a6bf-9ec041b75559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Pair 1:\n",
            "  Sentence A: The dog chased the ball into the yard.\n",
            "  Sentence B: It brought the ball back to its owner.\n",
            "  ➜ Prediction : coherent\n",
            "  ➜ Chosen tier: Fine-tuned NSP   (confidence=0.873)\n",
            "\n",
            "Pair 2:\n",
            "  Sentence A: I turned off the lights before going to bed.\n",
            "  Sentence B: Saturn’s rings are made mostly of ice.\n",
            "  ➜ Prediction : incoherent\n",
            "  ➜ Chosen tier: PROMPT   (confidence=0.800)\n",
            "\n",
            "Pair 3:\n",
            "  Sentence A: The class finished the science experiment.\n",
            "  Sentence B: Afterwards, the students wrote their lab reports.\n",
            "  ➜ Prediction : coherent\n",
            "  ➜ Chosen tier: Fine-tuned NSP   (confidence=0.829)\n"
          ]
        }
      ],
      "source": [
        "examples = [\n",
        "    # 1. Obviously coherent\n",
        "    (\"The dog chased the ball into the yard.\",\n",
        "     \"It brought the ball back to its owner.\"),\n",
        "\n",
        "    # 2. Clearly incoherent\n",
        "    (\"I turned off the lights before going to bed.\",\n",
        "     \"Saturn’s rings are made mostly of ice.\"),\n",
        "\n",
        "    # 3. Borderline / ambiguous\n",
        "    (\"The class finished the science experiment.\",\n",
        "     \"Afterwards, the students wrote their lab reports.\")\n",
        "]\n",
        "\n",
        "for i, (a, b) in enumerate(examples, 1):\n",
        "    pred, tier, conf = combined_predict(a, b)\n",
        "    print(f\"\\nPair {i}:\")\n",
        "    print(f\"  Sentence A: {a}\")\n",
        "    print(f\"  Sentence B: {b}\")\n",
        "    print(f\"  ➜ Prediction : {'coherent' if pred else 'incoherent'}\")\n",
        "    print(f\"  ➜ Chosen tier: {tier}   (confidence={conf:.3f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d54482f2-8269-4e63-b82f-6630ac92c911",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "a61626577304457dae18371bb23aff17"
          ]
        },
        "id": "d54482f2-8269-4e63-b82f-6630ac92c911",
        "outputId": "92034bdc-deca-4885-c052-46035b98d856"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a61626577304457dae18371bb23aff17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/15000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Combined model on Wiki-Paragraphs test set (1% sample)\n",
            "Accuracy : 0.6333\n",
            "Precision: 0.7373\n",
            "Recall   : 0.4126\n",
            "F1-score : 0.5291\n"
          ]
        }
      ],
      "source": [
        "#  Evaluate on the test split\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Evaluating\"):\n",
        "\n",
        "    pred, _, _ = combined_predict(row[\"sentence1\"], row[\"sentence2\"])\n",
        "    y_true.append(row[\"label\"])\n",
        "    y_pred.append(pred)\n",
        "\n",
        "acc  = accuracy_score(y_true, y_pred)\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\")\n",
        "\n",
        "print(\"\\nCombined model on Wiki-Paragraphs test set\")\n",
        "print(f\"Accuracy : {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall   : {rec:.4f}\")\n",
        "print(f\"F1-score : {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98279674-044e-4741-9833-a4d3357981eb",
      "metadata": {
        "id": "98279674-044e-4741-9833-a4d3357981eb"
      },
      "source": [
        "#### Results\n",
        "The final combined model was tested across three example pairs and the model was able to do NSP accurately. In one of the cases, fine fine-tuned model did not produce results with confidence, hence it fell back to a prompt-based NSP checker. Even after using the training set for building the FAISS index, the RAG based approach was not accurate enough.\n",
        "\n",
        "The combined model was also tested on the test set (15,000 sentence pairs).\n",
        "\n",
        "The model achieves a moderate accuracy of 63.33%, indicating that it correctly classifies about two-thirds of all sentence pairs. The precision of 73.73% is relatively high, suggesting that when your model predicts a positive relationship between sentences, it's correct about 74% of the time.\n",
        "\n",
        "However, the recall is notably lower at 41.26%, indicating that the model identifies less than half of the actual positive sentence pairs in the dataset. This substantial gap between precision and recall results in an F1-score of 52.91%. This performance pattern suggests that the combined model is more conservative in making positive predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44d685a9-b735-4a69-b22e-c229d4de7066",
      "metadata": {
        "id": "44d685a9-b735-4a69-b22e-c229d4de7066"
      },
      "source": [
        "## User Interaction: Try Your Own Sentences\n",
        "The final cell allows users to manually input two sentences and receive immediate feedback on whether the second sentence logically follows the first. This feature demonstrates how the model can be applied in real-world settings to assess sentence coherence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4a090fd-df98-4b7e-9455-b0b778030b06",
      "metadata": {
        "id": "f4a090fd-df98-4b7e-9455-b0b778030b06",
        "outputId": "579d8dee-2822-479f-b6bc-92aba8b8bb49"
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "Sentence A (first sentence):  Hi, I am Sudip!\n",
            "Sentence B (second sentence):  I learned a lot while doing this project!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌  Incoherent — the second sentence does NOT follow the first.\n"
          ]
        }
      ],
      "source": [
        "user_sentence_a = input(\"Sentence A (first sentence): \")\n",
        "user_sentence_b = input(\"Sentence B (second sentence): \")\n",
        "pred, _, _ = combined_predict(user_sentence_a, user_sentence_b)\n",
        "\n",
        "if pred:\n",
        "    print(\"Coherent — the second sentence logically follows the first.\")\n",
        "else:\n",
        "    print(\"❌  Incoherent — the second sentence does NOT follow the first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "821352de-78a0-498a-991d-400eaf03bfbb",
      "metadata": {
        "id": "821352de-78a0-498a-991d-400eaf03bfbb"
      },
      "source": [
        "## Use Case & Educational Value\n",
        "\n",
        "This interactive mode is particularly valuable for:\n",
        "\n",
        "- Children learning how to write coherent narratives or essays.\n",
        "\n",
        "- Beginner English learners practicing sentence order and logical flow.\n",
        "\n",
        "- Tutoring tools that offer instant, interpretable feedback for writing practice.\n",
        "\n",
        "- Language learning platforms that help users improve sentence construction.\n",
        "\n",
        "- Demo/testing purposes to showcase how sentence coherence can be modeled computationally.\n",
        "\n",
        "## Real-World Integration\n",
        "\n",
        "This logic could easily be embedded into a website or educational application, with a simple interface allowing users to:\n",
        "\n",
        "- Type two sentences,\n",
        "\n",
        "- Click a “Check Coherence” button,\n",
        "\n",
        "- And receive a verdict like:\n",
        "\n",
        "“Coherent — good continuation!”\n",
        "\n",
        "“Incoherent — try revising your second sentence.”\n",
        "\n",
        "Such tools can encourage self-guided learning, improve feedback loops for writing assignments, and introduce AI-powered writing aids into classrooms or online platforms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a0cf886-395b-46c2-a195-9545ac9b2f68",
      "metadata": {
        "id": "3a0cf886-395b-46c2-a195-9545ac9b2f68"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}